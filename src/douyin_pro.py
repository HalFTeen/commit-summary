"""Advanced Douyin comment scraper with robust extraction."""

import asyncio
import json
import re
import random
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from fake_useragent import UserAgent
from playwright.async_api import async_playwright


class DouyinScraperPro:
    """Professional Douyin comment scraper with advanced features."""

    def __init__(
        self,
        proxy_url: Optional[str] = None,
        headless: bool = False,
        timeout: int = 60,
    ):
        """Initialize advanced scraper.

        Args:
            proxy_url: Optional proxy URL
            headless: Run browser in headless mode
            timeout: Page load timeout in seconds
        """
        self.proxy_url = proxy_url
        self.headless = headless
        self.timeout = timeout * 1000  # Convert to ms
        self.user_agent = UserAgent().random
        self.comments = []
        self.seen_hashes = set()  # Track seen comments to avoid duplicates

    async def scrape(self, video_url: str, max_comments: Optional[int] = None) -> dict:
        """Scrape Douyin comments with full pipeline.

        Args:
            video_url: Douyin video URL
            max_comments: Maximum comments to scrape (None for all)

        Returns:
            Dictionary with scraping results
        """
        print("=" * 70)
        print("ğŸ” æŠ–éŸ³é«˜çº§è¯„è®ºçˆ¬è™«")
        print("=" * 70)
        print(f"\nğŸ“ ç›®æ ‡URL: {video_url}")
        print(f"ğŸ¥ è§†é¢‘ID: {self._extract_video_id(video_url)}")
        print(f"ğŸŒ ç”¨æˆ·ä»£ç†: {self.user_agent[:50]}...")
        print(f"ğŸ–¥ï¸  æ— å¤´æ¨¡å¼: {'æ˜¯' if self.headless else 'å¦'}")

        try:
            async with async_playwright() as p:
                # Launch browser with advanced options
                browser_args = [
                    "--disable-blink-features=AutomationControlled",
                    "--disable-dev-shm-usage",
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--start-maximized",
                ]

                launch_options = {
                    "headless": self.headless,
                    "args": browser_args,
                    "slow_mo": 50,  # Slow down operations
                }

                if self.proxy_url:
                    launch_options["proxy"] = {"server": self.proxy_url}

                print("\nğŸš€ å¯åŠ¨æµè§ˆå™¨...")
                browser = await p.chromium.launch(**launch_options)

                # Create context with realistic settings
                context = await browser.new_context(
                    user_agent=self.user_agent,
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-CN",
                    timezone_id="Asia/Shanghai",
                    permissions=["geolocation"],
                    geolocation={"latitude": 39.9042, "longitude": 116.4074},  # Beijing
                    color_scheme="light",
                    device_scale_factor=1,
                )

                # Inject anti-detection scripts
                await context.add_init_script("""
                    // Hide webdriver
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });

                    // Mock Chrome object
                    window.chrome = {
                        runtime: {},
                        loadTimes: function() {},
                        csi: function() {},
                        app: {}
                    };

                    // Mock permissions
                    const originalQuery = window.navigator.permissions.query;
                    window.navigator.permissions.query = (parameters) => (
                        parameters.name === 'notifications' ?
                            Promise.resolve({ state: Notification.permission }) :
                            originalQuery(parameters)
                    );

                    // Mock plugins
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });

                    // Mock languages
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-CN', 'zh', 'en-US', 'en']
                    });
                """)

                page = await context.new_page()

                # Set up console logging
                page.on("console", lambda msg: print(f"ğŸ”§ [æµè§ˆå™¨] {msg.text}"))

                print("ğŸ“„ æ­£åœ¨åŠ è½½é¡µé¢...")
                try:
                    await page.goto(video_url, timeout=self.timeout, wait_until="networkidle")
                except Exception as e:
                    print(f"âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å°è¯•...")
                    await page.wait_for_timeout(5000)

                # Wait for page to stabilize
                await page.wait_for_timeout(random.randint(3000, 5000))

                # Check if we need to handle any popups or verifications
                await self._handle_page_elements(page)

                # Scroll to find and load comments
                print("\nğŸ”„ å¼€å§‹æ»šåŠ¨å’ŒåŠ è½½è¯„è®º...")
                scroll_attempts = 0
                max_scrolls = 100
                consecutive_empty = 0

                while scroll_attempts < max_scrolls:
                    if max_comments and len(self.comments) >= max_comments:
                        print(f"\nâœ… å·²è¾¾åˆ°ç›®æ ‡è¯„è®ºæ•°: {max_comments}")
                        break

                    # Extract comments from current view
                    new_comments = await self._extract_comments_advanced(page)
                    if new_comments:
                        old_count = len(self.comments)
                        self.comments.extend(new_comments)
                        new_count = len(self.comments)
                        consecutive_empty = 0
                        print(f"ğŸ“ ç¬¬{scroll_attempts + 1}æ¬¡æ»šåŠ¨: +{new_count - old_count} æ¡ (æ€»è®¡: {new_count})")
                    else:
                        consecutive_empty += 1
                        print(f"â¸ï¸  ç¬¬{scroll_attempts + 1}æ¬¡æ»šåŠ¨: æ— æ–°è¯„è®º")

                        if consecutive_empty >= 5:
                            print("\nâ„¹ï¸  è¿ç»­å¤šæ¬¡æ— æ–°è¯„è®ºï¼Œå¯èƒ½å·²åˆ°åº•éƒ¨")
                            break

                    # Scroll down
                    await self._smart_scroll(page)
                    await page.wait_for_timeout(random.randint(1500, 2500))

                    scroll_attempts += 1

                await browser.close()

                # Prepare results
                video_id = self._extract_video_id(video_url)

                result = {
                    "video_url": video_url,
                    "video_id": video_id,
                    "total_comments": len(self.comments),
                    "comments": self.comments,
                    "scraped_at": datetime.now().isoformat(),
                    "scraper_version": "1.0.0",
                    "success": len(self.comments) > 0,
                }

                # Save results
                output_file = Path("output") / f"douyin_{video_id}_{int(datetime.now().timestamp())}.json"
                output_file.parent.mkdir(parents=True, exist_ok=True)

                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

                # Display summary
                self._print_summary(result, output_file)

                return result

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def _handle_page_elements(self, page):
        """Handle any popups, cookies, or verification elements."""
        print("ğŸ” æ£€æŸ¥é¡µé¢å…ƒç´ ...")

        # Try to find and click on any cookie consent
        cookie_selectors = [
            'button[class*="cookie"]',
            'button[class*="accept"]',
            'div[class*="cookie"] button',
            '[role="dialog"] button',
        ]

        for selector in cookie_selectors:
            try:
                elements = await page.locator(selector).all()
                if elements:
                    print(f"âœ“ æ‰¾åˆ°cookieæŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»: {selector}")
                    for element in elements[:1]:
                        await element.click()
                        await page.wait_for_timeout(1000)
                    break
            except:
                continue

    async def _smart_scroll(self, page):
        """Perform smart scrolling to load content."""
        try:
            # Get current scroll position
            scroll_height = await page.evaluate("document.body.scrollHeight")

            # Scroll to near bottom
            await page.evaluate(f"window.scrollTo(0, {scroll_height - 500})")

            # Wait for new content
            await page.wait_for_timeout(500)

        except Exception as e:
            print(f"âš ï¸  æ»šåŠ¨é”™è¯¯: {e}")
            try:
                # Fallback: use keyboard
                await page.keyboard.press("End")
                await page.wait_for_timeout(1000)
            except:
                pass

    async def _extract_comments_advanced(self, page) -> List[dict]:
        """Advanced comment extraction with multiple strategies."""
        new_comments = []

        try:
            # Strategy 1: Try to find comment containers
            comment_selectors = [
                'div[class*="comment-list"] > div',
                'div[class*="CommentItem"]',
                'div[class*="commentItem"]',
                'div[class*="reply-item"]',
                '[class*="CommentContainer"] > div',
                'li[class*="comment"]',
            ]

            all_text_content = await page.evaluate("""() => {
                const results = [];

                // Find all text elements that might be comments
                const allElements = document.querySelectorAll('*');

                for (let el of allElements) {
                    // Check if element contains text
                    const text = el.textContent?.trim();
                    if (!text || text.length < 2 || text.length > 1000) continue;

                    // Check if it looks like a comment text
                    const parent = el.parentElement;
                    if (!parent) continue;

                    // Check for common comment indicators
                    const hasLikeButton = parent.querySelector('[class*="like"], [class*="digg"], [class*="thumb"]');
                    const hasReplyButton = parent.querySelector('[class*="reply"], [class*="comment"]');
                    const isInCommentSection = parent.closest('[class*="comment"], [class*="Comment"]');

                    if (hasLikeButton || hasReplyButton || isInCommentSection) {
                        // Get parent element
                        const commentContainer = el.closest('[class*="comment"], [class*="Comment"], [class*="item"]');

                        if (commentContainer) {
                            // Try to extract likes
                            let likes = 0;
                            const likeElements = commentContainer.querySelectorAll('[class*="like"], [class*="digg"], [class*="count"]');
                            likeElements.forEach(likeEl => {
                                const likeText = likeEl.textContent || '';
                                const match = likeText.match(/\\d+/);
                                if (match) likes = Math.max(likes, parseInt(match[0]));
                            });

                            // Try to extract user info
                            let username = 'unknown';
                            const userElements = commentContainer.querySelectorAll('[class*="user"], [class*="name"], [class*="author"]');
                            userElements.forEach(userEl => {
                                const userText = userEl.textContent?.trim();
                                if (userText && userText.length > 0 && userText.length < 50) {
                                    username = userText;
                                }
                            });

                            results.push({
                                text: text,
                                likes: likes,
                                username: username,
                                html: commentContainer.outerHTML.substring(0, 500)
                            });
                        }
                    }
                }

                return results;
            }""")

            # Process results and deduplicate
            for comment_data in all_text_content:
                text = comment_data.get("text", "").strip()

                if not text or len(text) < 2:
                    continue

                # Create hash to avoid duplicates
                text_hash = hash(text)

                if text_hash in self.seen_hashes:
                    continue

                self.seen_hashes.add(text_hash)

                new_comments.append({
                    "text": text,
                    "likes": comment_data.get("likes", 0),
                    "username": comment_data.get("username", "anonymous"),
                    "timestamp": datetime.now().isoformat(),
                })

        except Exception as e:
            print(f"âš ï¸  æå–è¯„è®ºå¤±è´¥: {e}")

        return new_comments

    def _extract_video_id(self, url: str) -> str:
        """Extract video ID from Douyin URL."""
        try:
            if "modal_id=" in url:
                return url.split("modal_id=")[-1].split("&")[0]
            elif "/video/" in url:
                return url.split("/video/")[-1].split("?")[0]
            else:
                return "unknown"
        except:
            return "unknown"

    def _print_summary(self, result: dict, output_file: Path):
        """Print summary of scraping results."""
        print("\n" + "=" * 70)
        print("ğŸ“Š çˆ¬å–ç»“æœæ‘˜è¦")
        print("=" * 70)

        print(f"\nâœ… çŠ¶æ€: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ“ è¯„è®ºæ•°é‡: {result['total_comments']}")
        print(f"ğŸ¥ è§†é¢‘ ID: {result['video_id']}")

        if result['comments']:
            total_likes = sum(c.get('likes', 0) for c in result['comments'])
            avg_likes = total_likes / len(result['comments']) if result['comments'] else 0

            print(f"ğŸ‘ æ€»ç‚¹èµæ•°: {total_likes}")
            print(f"ğŸ“ˆ å¹³å‡ç‚¹èµ: {avg_likes:.1f}")

            # Show top 5 comments by likes
            top_comments = sorted(
                result['comments'], key=lambda x: x.get('likes', 0), reverse=True
            )[:5]

            print(f"\nğŸ’¬ çƒ­é—¨è¯„è®º (Top 5):")
            for i, comment in enumerate(top_comments, 1):
                print(f"   {i}. [{comment.get('likes', 0)} èµ] {comment.get('text', '')[:80]}")
                if comment.get('username'):
                    print(f"      ğŸ‘¤ {comment.get('username', 'anonymous')}")

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("=" * 70)

        if result['total_comments'] == 0:
            print("\nâš ï¸  æœªè·å–åˆ°è¯„è®ºï¼Œå¯èƒ½åŸå› :")
            print("   1. éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹è¯„è®º")
            print("   2. è§†é¢‘å·²åˆ é™¤æˆ–ä¸å­˜åœ¨")
            print("   3. åçˆ¬è™«æ£€æµ‹ï¼Œå»ºè®®:")
            print("      - ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•")
            print("      - ä½¿ç”¨ä»£ç†IP")
            print("      - é™ä½æ»šåŠ¨é€Ÿåº¦")
            print("   4. é¡µé¢ç»“æ„å·²å˜åŒ–ï¼Œéœ€è¦æ›´æ–°é€‰æ‹©å™¨")


async def main():
    """Main entry point."""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 douyin_pro.py <æŠ–éŸ³è§†é¢‘URL> [é€‰é¡¹]")
        print("\né€‰é¡¹:")
        print("  --headless     æ— å¤´æ¨¡å¼è¿è¡Œï¼ˆé»˜è®¤ï¼šæ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰")
        print("  --max N        æœ€å¤šçˆ¬å–Næ¡è¯„è®º")
        print("  --proxy URL     ä½¿ç”¨ä»£ç†")
        print("\nç¤ºä¾‹:")
        print("python3 douyin_pro.py 'https://www.douyin.com/user/...?modal_id=7597795827700487787'")
        print("python3 douyin_pro.py '<URL>' --max 50 --headless")
        return

    url = sys.argv[1]

    # Parse options
    headless = "--headless" in sys.argv
    max_comments = None

    for i, arg in enumerate(sys.argv):
        if arg == "--max" and i + 1 < len(sys.argv):
            try:
                max_comments = int(sys.argv[i + 1])
            except ValueError:
                pass

    # Create scraper
    scraper = DouyinScraperPro(headless=headless)

    # Run scraping
    try:
        result = await scraper.scrape(url, max_comments=max_comments)

        if result['success']:
            print("\nâœ… çˆ¬å–æˆåŠŸå®Œæˆï¼")
            return 0
        else:
            print("\nâŒ æœªèƒ½è·å–åˆ°è¯„è®º")
            return 1

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
